{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 3 - Which book would you recommend?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing related libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "import warnings\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.firefox import GeckoDriverManager\n",
    "from langdetect import detect\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import PorterStemmer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. Get the list of books"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* From [best books ever list](https://www.goodreads.com/list/show/1.Best_Books_Ever?page=1) we want to collect the url associated to each book in the list and retrieve only the urls of the books listed in the first 300 pages.\n",
    "\n",
    "\n",
    "\n",
    "* The output of this step is a `.txt` file whose single line corresponds to a book's url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inizialize an empty list\n",
    "url = []\n",
    "\n",
    "#for each page, save the corresponding web page, find the anchor elements and save the corresponding tags  \n",
    "for i in range(1, 301):\n",
    "    page = requests.get(\"https://www.goodreads.com/list/show/1.Best_Books_Ever?page=\"+ str(i))\n",
    "    soup = BeautifulSoup(page.content, features='lxml')\n",
    "    tag_a = soup.find_all('a', {\"class\": \"bookTitle\"}, itemprop = \"url\")\n",
    "    \n",
    "#for each book, save the corresponding url into the array\n",
    "    for j in range(0,100):\n",
    "        url.append(\"https://www.goodreads.com\"+ tag_a[j]['href'])\n",
    "        \n",
    "#create a txt file where for each row there is a book's url \n",
    "with open(\"url.txt\", 'w') as f:\n",
    "    f.write(\"\\n\".join(map(str, url)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. Crawl books"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Download the html corresponding to each of the collected urls.\n",
    "\n",
    "\n",
    "2. After you collect a single page, immediatly save its `html` in a file. In this way, if your program stops, for any reason, you will not loose the data collected up to the stopping point.\n",
    "\n",
    "\n",
    "3. Organize the entire set of downloaded `html` pages into folders. Each folder will contain the `htmls` of the books in page 1, page 2, ... of the list of books."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warning :\n",
    "\n",
    "### Do not run the below cell because it'll download over 20 GB files !\n",
    "#### Before running modify the `for` loop range to a lower number (e.g. 3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we open the url.txt file, reading the lines and then close the file\n",
    "f = open(\"url.txt\", \"r\")\n",
    "lines = f.readlines()\n",
    "f.close()\n",
    "\n",
    "#Setting our parent directory and the directory associated with each page number from the list\n",
    "## PLEASE CHANGE THE PARENT DIRECTORY ACCORDING TO YOUR SYSTEM\n",
    "parent_dir = \"C:/Users/engme/OneDrive/Desktop/ALL Materials/Data Science - Sapienza/1st Semester/ALGORITHMIC METHODS OF DATA MINING AND LABORATORY/Labs/6/Links\"\n",
    "directory = \"Page_\"\n",
    "page_num = 0\n",
    "\n",
    "\n",
    "#Looping for 300 times that corresponds to the number of pages\n",
    "for i in range(300):\n",
    "    #Incrementing page number according to each page\n",
    "    page_num += 1\n",
    "    #Setting the current working directory\n",
    "    directory = \"Page_\" + str(page_num)\n",
    "    #Setting the main path to create the directory\n",
    "    path = os.path.join(parent_dir, directory)\n",
    "    #Creating new directory\n",
    "    os.makedirs(path)\n",
    "    \n",
    "    #Looping for 100 times, which is the number of articles per page\n",
    "    for i in range(100):\n",
    "        #Selecting the corresponding link\n",
    "        link = lines[i][:-1]\n",
    "        #Dowloading the article\n",
    "        r = requests.get(link, allow_redirects=True)\n",
    "        #Setting the name file to keep the track\n",
    "        file_name = \"article_\" + str(i+1) + \".html\"\n",
    "        #Saving the html file to its corresponding directory\n",
    "        open(parent_dir + \"/\" + directory + \"/\" + file_name, 'wb').write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Parse downloaded pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Extracting the books information for each book as following:\n",
    "\n",
    "1. Title (to save as `bookTitle`)\n",
    "2. Series (to save as `bookSeries`)\n",
    "3. Author(s), the first box in the picture below (to save as `bookAuthors`)\n",
    "4. Ratings, average stars (to save as `ratingValue`)\n",
    "5. Number of givent ratings (to save as `ratingCount`)\n",
    "6. Number of reviews (to save as `reviewCount`)\n",
    "7. The entire plot (to save as `Plot`)\n",
    "8. Number of pages (to save as `NumberofPages`)\n",
    "9. Published (Publishing Date)\n",
    "10. Characters\n",
    "11. Setting\n",
    "12. Url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Custom function designed to \n",
    "parent_dir = \"C:/Users/engme/OneDrive/Desktop/ALL Materials/Data Science - Sapienza/1st Semester/ALGORITHMIC METHODS OF DATA MINING AND LABORATORY/Labs/6\"\n",
    "\n",
    "functions.info_parser(parent_dir, pages=300, tsv_articles= \"tsv_articles\", links= \"Links\", url= 'url')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All tsv files generated sucessfully in tsv_articles directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Search Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to create two different Search Engines that, given as input a query, return the books that match the query.\n",
    "\n",
    "First, you must pre-process all the information collected for each book by :\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Removing stopwords\n",
    "2. Removing punctuation\n",
    "3. Stemming\n",
    "4. Anything else you think it's needed\n",
    "\n",
    "\n",
    "For this purpose, you can use the [nltk](https://www.nltk.org/) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#initialize an empty dictionary\n",
    "processed_docs = {}\n",
    "\n",
    "parent_dir = \"C:\\\\Users\\\\elisa\\\\Desktop\\\\Algorithmic Methods of Data Mining\\\\ADM-HW3\\\\tsv_articles\\\\tsv_articles\"\n",
    "\n",
    "#initialize a list that will contain the lists of tokens, one per plot \n",
    "tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "stop_words = set(stopwords.words('english')) \n",
    "ps = PorterStemmer()\n",
    "\n",
    "#for every article_i.tsv file, extract the Plot, tokenize it and preprocess it \n",
    "for n_art in range(1, 30001):\n",
    "    directory = \"article_\" + str(n_art) + \".tsv\"\n",
    "    path = os.path.join(parent_dir, directory)\n",
    "    if os.path.exists(path):\n",
    "        plot = pd.read_csv(path, delimiter = '\\t', usecols = ['Plot'])\n",
    "        tokens = tokenizer.tokenize(str(plot))\n",
    "        processed_doc = []\n",
    "        for token in tokens:\n",
    "            if (token != 'Plot') & (token != '0') & (not token in stop_words):\n",
    "                processed_doc.append(ps.stem(token))\n",
    "\n",
    "        processed_docs['document_'+str(n_art)] = processed_doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize an empty dictionary\n",
    "vocabulary = {}\n",
    "\n",
    "term_id = 1\n",
    "#for every document (for every plot in our case)\n",
    "for doc in processed_docs.values():\n",
    "    \n",
    "#for every token in the document:\n",
    "    for tok in doc:\n",
    "\n",
    "#if the token is not present in the dictionary yet...\n",
    "        if tok not in vocabulary:\n",
    "        \n",
    "#...add it and set term_id as his id\n",
    "            vocabulary[tok] =  term_id,\n",
    "            term_id += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.1. Conjunctive query**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.1.1) Create your index!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#initialize an empty dictionary\n",
    "inv_index1 = {}\n",
    "\n",
    "doc_id = 0 \n",
    "#for every document (for every plot in our case)\n",
    "for doc in processed_docs.values():\n",
    "\n",
    "    #increase the id of the document\n",
    "    doc_id+=1\n",
    "    \n",
    "    directory = \"article_\" + str(doc_id) + \".tsv\"\n",
    "    path = os.path.join(parent_dir, directory)\n",
    "    if os.path.exists(path):\n",
    "        \n",
    "    #for every token in the document\n",
    "        for tok in doc:\n",
    "\n",
    "    #if the id of that specific token is not present in the dictionary yet...\n",
    "            if vocabulary[tok] not in inv_index1:\n",
    "\n",
    "    #...add it to the dictionary as a key and let document_doc_id be one of its values:\n",
    "                inv_index1[vocabulary[tok]] = [\"document_\"+str(doc_id)]\n",
    "\n",
    "    #else, if this token is present in the dictionary but document_doc_id is not one of his values yet...\n",
    "            elif \"document_\"+str(doc_id) not in inv_index1.get(vocabulary[tok]):\n",
    "\n",
    "    #append document_doc_id to his values\n",
    "                inv_index1[vocabulary[tok]].append(\"document_\"+str(doc_id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "my_dict = {}\n",
    "for i in range(1,30001):\n",
    "    directory = \"article_\" + str(i) + \".tsv\"\n",
    "    path = os.path.join(parent_dir, directory)\n",
    "    if os.path.exists(path):\n",
    "        my_dict[\"document_\"+str(i)] = \"article_\"+str(i)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.1.2) Execute the query**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Search_Engine1(query):\n",
    "    \n",
    "    #stem the tokens of the query in order to create a new query: my_query\n",
    "    my_query = []\n",
    "    for tok in query:\n",
    "        my_query.append(ps.stem(tok))\n",
    "\n",
    "    #create a new dictionary which contains just the keys present in my_query        \n",
    "    my_invertedId = {}\n",
    "    for tok in my_query:\n",
    "        if tok in vocabulary:\n",
    "            my_invertedId[tok] = inv_index1.get(vocabulary[tok])\n",
    "            \n",
    "    #if any of the query's tokens is not present into the vocabulary, give an Error Message to the user\n",
    "        elif tok not in vocabulary:\n",
    "            return(\"The query is not present in any plot\")\n",
    "      \n",
    "    #define a list of sets where each set represents the documents that contain each token of the query\n",
    "    my_sets = []\n",
    "    for key in my_invertedId:\n",
    "        my_sets.append(set(my_invertedId[key]))\n",
    "    result = set()\n",
    "\n",
    "    for i in range(1, 30001):\n",
    "        result.add('document_'+str(i))\n",
    "        \n",
    "    for my_set in my_sets:\n",
    "        result = result.intersection(my_set)\n",
    "    \n",
    "    if result == set():\n",
    "        return(\"The query is not present in any plot\")\n",
    "    else:\n",
    "        found = list(result)\n",
    "\n",
    "        i = 0\n",
    "        for item in found:\n",
    "            directory = my_dict[item]+\".tsv\"\n",
    "            path = os.path.join(parent_dir, directory)\n",
    "            if i == 0:\n",
    "                data = pd.read_csv(path, delimiter = '\\t', usecols = ['bookTitle', 'Plot', 'Url'])\n",
    "            else:\n",
    "                data = data.append(pd.read_csv(path, delimiter = '\\t', usecols = ['bookTitle', 'Plot', 'Url']))\n",
    "               \n",
    "            data = data.rename(index = {0:'book_'+str(i+1)})\n",
    "            \n",
    "            i+=1\n",
    "                            \n",
    "        return(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Search_Engine1(input().split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.2) Conjunctive query & Ranking score**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.2.1) Inverted index**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#initialize an empty dictionary\n",
    "inv_index2 = {}\n",
    "\n",
    "n = len(processed_docs) # number of documents\n",
    "\n",
    "#for every term_id belonging to the dictionary my_dict\n",
    "for term_id in inv_index1:\n",
    "    line = [term_id, inv_index1[term_id]]\n",
    "    N = len(inv_index1[term_id]) #number of documents with the term corrisponding to the id term_id\n",
    "    \n",
    "    #for every document that contains that term\n",
    "    for doc in line[1]:\n",
    "        tf = processed_docs[doc].count(list(vocabulary.keys())[term_id-1]) #term frequency\n",
    "        Idf = np.log(n/N) #inverse document frequency\n",
    "        tfIdf = tf*Idf\n",
    "        \n",
    "        if term_id not in inv_index2:\n",
    "            inv_index2[term_id] = [(doc,tfIdf)]\n",
    "        else:\n",
    "            inv_index2[term_id].append((doc,tfIdf))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.2.2) Execute the query**\n",
    "In order to detect the top-k books we can ignore ||q||, defined as the norm of the number of times a term appears in the query, because it is constant with respect to the Cosine Similarity. Before defining the second Search Engine, we calculate ||Di|| for i in 1,...,n, defined as the norm of the tdIdf scores of the i-th book, where n  is the number of books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize an empty dictionary D that will contain for each document the norm of its tfIdf\n",
    "D = {}\n",
    "\n",
    "#for every document\n",
    "for document in my_dict:\n",
    "\n",
    "    for key in inv_index2:\n",
    "     \n",
    "        for doc in inv_index2[key]:\n",
    "        \n",
    "            if doc[0] == document:\n",
    "                \n",
    "                if document not in D:\n",
    "                    \n",
    "                    D[document] = [doc[1]]\n",
    "\n",
    "                else:\n",
    "                    \n",
    "                    D[document].append(doc[1])  \n",
    "                    \n",
    "    if document in D:\n",
    "        \n",
    "        D[document] = np.linalg.norm(D[document])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings. filterwarnings('ignore')\n",
    "\n",
    "def Search_Engine2(query):\n",
    "    \n",
    "    #stem the tokens of the query in order to create a new query: my_query\n",
    "    my_query = []\n",
    "    for tok in query:\n",
    "        my_query.append(ps.stem(tok))\n",
    "\n",
    "    #create a new dictionary which contains just the keys present in my_query        \n",
    "    my_invertedId = {}\n",
    "    for tok in my_query:\n",
    "        if tok in vocabulary:\n",
    "            my_invertedId[tok] = inv_index1.get(vocabulary[tok])\n",
    "\n",
    "    #if any of the query's tokens is not present into the vocabulary, give an Error Message to the user\n",
    "        elif tok not in vocabulary:\n",
    "            return(\"The query is not present in any plot\")\n",
    "      \n",
    "    #define a list of sets where each set represents the documents that contain each token of the query\n",
    "    my_sets = []\n",
    "    for key in my_invertedId:\n",
    "        my_sets.append(set(my_invertedId[key]))\n",
    "    result = set()\n",
    "\n",
    "    for i in range(1, 30001):\n",
    "        result.add('document_'+str(i))\n",
    "        \n",
    "    for my_set in my_sets:\n",
    "        result = result.intersection(my_set)\n",
    "    \n",
    "    if result == set():\n",
    "        return(\"The query is not present in any plot\")\n",
    "    else:\n",
    "        found = list(result)\n",
    "\n",
    "        i = 0\n",
    "        for item in found:\n",
    "            \n",
    "            similarity = 0\n",
    "            for term in my_query:\n",
    "                \n",
    "                for doc in inv_index2[vocabulary[term]]:\n",
    "        \n",
    "                    if doc[0] == item:\n",
    "                    \n",
    "                        if D[item] != 0:\n",
    "\n",
    "                            similarity += doc[1]/D[item]\n",
    "                            \n",
    "            similarity = round(similarity, 2)\n",
    "            directory = my_dict[item]+\".tsv\"\n",
    "            path = os.path.join(parent_dir, directory)\n",
    "            \n",
    "            if i == 0:\n",
    "                data = pd.read_csv(path, delimiter = '\\t', usecols = ['bookTitle', 'Plot', 'Url'])\n",
    "                data['Similarity'] = similarity\n",
    "                \n",
    "            else:\n",
    "                data = data.append(pd.read_csv(path, delimiter = '\\t', usecols = ['bookTitle', 'Plot', 'Url']))\n",
    "                data['Similarity'][0] = similarity\n",
    "            \n",
    "            data.sort_values(by = 'Similarity', inplace = True, ascending = False)\n",
    "            data = data.rename(index = {0:'book_'+str(i+1)})\n",
    "            \n",
    "            i+=1\n",
    "            \n",
    "        return(data[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "could\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bookTitle</th>\n",
       "      <th>Plot</th>\n",
       "      <th>Url</th>\n",
       "      <th>Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>book_21</th>\n",
       "      <td>The Peacemaker</td>\n",
       "      <td>With war scars that no one could see and that ...</td>\n",
       "      <td>https://www.goodreads.com/book/show/662.Atlas_...</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>book_53</th>\n",
       "      <td>The Hunger Games</td>\n",
       "      <td>Could you survive on your own in the wild, wit...</td>\n",
       "      <td>https://www.goodreads.com/book/show/2767052-th...</td>\n",
       "      <td>0.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>book_76</th>\n",
       "      <td>Shadow Kiss</td>\n",
       "      <td>WHAT IF FOLLOWING HER HEART MEANS ROSE COULD L...</td>\n",
       "      <td>https://www.goodreads.com/book/show/19543.Wher...</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                bookTitle                                               Plot  \\\n",
       "book_21    The Peacemaker  With war scars that no one could see and that ...   \n",
       "book_53  The Hunger Games  Could you survive on your own in the wild, wit...   \n",
       "book_76       Shadow Kiss  WHAT IF FOLLOWING HER HEART MEANS ROSE COULD L...   \n",
       "\n",
       "                                                       Url  Similarity  \n",
       "book_21  https://www.goodreads.com/book/show/662.Atlas_...        1.00  \n",
       "book_53  https://www.goodreads.com/book/show/2767052-th...        0.49  \n",
       "book_76  https://www.goodreads.com/book/show/19543.Wher...        0.32  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Search_Engine2(input().split())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
