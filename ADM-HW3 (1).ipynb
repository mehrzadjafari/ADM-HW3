{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 3 - Which book would you recommend?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing related libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. Get the list of books"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* From [best books ever list](https://www.goodreads.com/list/show/1.Best_Books_Ever?page=1) we want to collect the url associated to each book in the list and retrieve only the urls of the books listed in the first 300 pages.\n",
    "\n",
    "\n",
    "\n",
    "* The output of this step is a `.txt` file whose single line corresponds to a book's url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inizialize an empty list\n",
    "url = []\n",
    "\n",
    "#for each page, save the corresponding web page, find the anchor elements and save the corresponding tags  \n",
    "for i in range(1, 301):\n",
    "    page = requests.get(\"https://www.goodreads.com/list/show/1.Best_Books_Ever?page=\"+ str(i))\n",
    "    soup = BeautifulSoup(page.content, features='lxml')\n",
    "    tag_a = soup.find_all('a', {\"class\": \"bookTitle\"}, itemprop = \"url\")\n",
    "    \n",
    "#for each book, save the corresponding url into the array\n",
    "    for j in range(0,100):\n",
    "        url.append(\"https://www.goodreads.com\"+ tag_a[j]['href'])\n",
    "        \n",
    "#create a txt file where for each row there is a book's url \n",
    "with open(\"url.txt\", 'w') as f:\n",
    "    f.write(\"\\n\".join(map(str, url)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. Crawl books"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Download the html corresponding to each of the collected urls.\n",
    "\n",
    "\n",
    "* After you collect a single page, immediatly save its `html` in a file. In this way, if your program stops, for any reason, you will not loose the data collected up to the stopping point.\n",
    "\n",
    "\n",
    "* Organize the entire set of downloaded `html` pages into folders. Each folder will contain the `htmls` of the books in page 1, page 2, ... of the list of books."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warning :\n",
    "\n",
    "### Do not run the below cell because it'll download over 20 GB files !\n",
    "#### Before running modify the `for` loop range to a lower number (e.g. 3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we open the url.txt file, reading the lines and then close the file\n",
    "f = open(\"url.txt\", \"r\")\n",
    "lines = f.readlines()\n",
    "f.close()\n",
    "\n",
    "#Setting our parent directory and the directory associated with each page number from the list\n",
    "## PLEASE CHANGE THE PARENT DIRECTORY ACCORDING TO YOUR SYSTEM\n",
    "parent_dir = \"C:/Users/engme/OneDrive/Desktop/ALL Materials/Data Science - Sapienza/1st Semester/ALGORITHMIC METHODS OF DATA MINING AND LABORATORY/Labs/6/Links\"\n",
    "directory = \"Page_\"\n",
    "page_num = 0\n",
    "\n",
    "\n",
    "#Looping for 300 times that corresponds to the number of pages\n",
    "for i in range(300):\n",
    "    #Incrementing page number according to each page\n",
    "    page_num += 1\n",
    "    #Setting the current working directory\n",
    "    directory = \"Page_\" + str(page_num)\n",
    "    #Setting the main path to create the directory\n",
    "    path = os.path.join(parent_dir, directory)\n",
    "    #Creating new directory\n",
    "    os.makedirs(path)\n",
    "    \n",
    "    #Looping for 100 times, which is the number of articles per page\n",
    "    for i in range(100):\n",
    "        #Selecting the corresponding link\n",
    "        link = lines[i][:-1]\n",
    "        #Dowloading the article\n",
    "        r = requests.get(link, allow_redirects=True)\n",
    "        #Setting the name file to keep the track\n",
    "        file_name = \"article_\" + str(i+1) + \".html\"\n",
    "        #Saving the html file to its corresponding directory\n",
    "        open(parent_dir + \"/\" + directory + \"/\" + file_name, 'wb').write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Parse downloaded pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Extracting the books information for each book as following:\n",
    "\n",
    "1. Title (to save as `bookTitle`)\n",
    "2. Series (to save as `bookSeries`)\n",
    "3. Author(s), the first box in the picture below (to save as `bookAuthors`)\n",
    "4. Ratings, average stars (to save as `ratingValue`)\n",
    "5. Number of givent ratings (to save as `ratingCount`)\n",
    "6. Number of reviews (to save as `reviewCount`)\n",
    "7. The entire plot (to save as `Plot`)\n",
    "8. Number of pages (to save as `NumberofPages`)\n",
    "9. Published (Publishing Date)\n",
    "10. Characters\n",
    "11. Setting\n",
    "12. Url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Custom function designed to \n",
    "parent_dir = \"C:/Users/engme/OneDrive/Desktop/ALL Materials/Data Science - Sapienza/1st Semester/ALGORITHMIC METHODS OF DATA MINING AND LABORATORY/Labs/6\"\n",
    "\n",
    "functions.info_parser(parent_dir, pages=300, tsv_articles= \"tsv_articles\", links= \"Links\", url= 'url')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All tsv files generated sucessfully in tsv_articles directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Search Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to create two different Search Engines that, given as input a query, return the books that match the query.\n",
    "\n",
    "First, you must pre-process all the information collected for each book by :\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Removing stopwords\n",
    "2. Removing punctuation\n",
    "3. Stemming\n",
    "4. Anything else you think it's needed\n",
    "\n",
    "\n",
    "For this purpose, you can use the [nltk](https://www.nltk.org/) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parent_dir = \"C:\\\\Users\\\\elisa\\\\Desktop\\\\Algorithmic Methods of Data Mining\\\\ADM-HW3\\\\tsv_articles\\\\tsv_articles\"\n",
    "\n",
    "#initialize a list that will contain the lists of tokens, one per plot \n",
    "processed_docs = []\n",
    "tokenizer = functions.nltk.RegexpTokenizer(r\"\\w+\")\n",
    "stop_words = set(functions.stopwords.words('english')) \n",
    "ps = functions.PorterStemmer()\n",
    "\n",
    "#for every article_i.tsv file, extract the Plot, tokenize it and preprocess it \n",
    "for n_art in range(1, 30001):\n",
    "    directory = \"article_\" + str(n_art) + \".tsv\"\n",
    "    path = functions.os.path.join(parent_dir, directory)\n",
    "    if functions.os.path.exists(path):\n",
    "        plot = functions.pd.read_csv(path, delimiter = '\\t', usecols = ['Plot'])\n",
    "        tokens = tokenizer.tokenize(str(plot))\n",
    "        processed_doc = []\n",
    "        for token in tokens:\n",
    "            if (token != 'Plot') & (token != '0') & (not token in stop_words):\n",
    "                processed_doc.append(ps.stem(token))\n",
    "\n",
    "        processed_docs.append(processed_doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#initialize an empty dictionary\n",
    "vocabulary = {}\n",
    "\n",
    "term_id = 1\n",
    "\n",
    "#for every document (for every plot in our case)\n",
    "for doc in processed_docs:\n",
    "    \n",
    "#for every token in the document\n",
    "    for tok in doc:\n",
    "        \n",
    "#if the token is not present in the dictionary yet...\n",
    "        if tok not in vocabulary:\n",
    "        \n",
    "#...add it and set term_id as his id\n",
    "            vocabulary[tok] =  term_id\n",
    "            term_id += 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#initialize an empty dictionary\n",
    "my_dict = {}\n",
    "\n",
    "doc_id = 0\n",
    "n_art= 1 \n",
    "#for every document (for every plot in our case)\n",
    "for doc in processed_docs:\n",
    "    directory = \"article_\" + str(n_art) + \".tsv\"\n",
    "    path = functions.os.path.join(parent_dir, directory)\n",
    "    if functions.os.path.exists(path):\n",
    "        \n",
    "    #increase the id of the document\n",
    "        doc_id+=n_art\n",
    "        n_tok = 0\n",
    "\n",
    "    #for every token in the document\n",
    "        for tok in doc:\n",
    "\n",
    "    #if the id of that specific token is not present in the dictionary yet...\n",
    "            if vocabulary[tok] not in my_dict:\n",
    "\n",
    "    #...add it to the dictionary as a key and let document_doc_id be one of its values:\n",
    "                my_dict[vocabulary[tok]] = [\"document_\"+str(doc_id)]\n",
    "\n",
    "    #else, if this token is present in the dictionary but document_doc_id is not one of his values yet...\n",
    "            elif \"document_\"+str(doc_id) not in my_dict.get(vocabulary[tok]):\n",
    "\n",
    "    #append document_doc_id to his values\n",
    "                my_dict[vocabulary[tok]].append(\"document_\"+str(doc_id))\n",
    "    n_art += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "could\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bookTitle</th>\n",
       "      <th>Plot</th>\n",
       "      <th>Url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>book_1</th>\n",
       "      <td>Dreams and Shadows</td>\n",
       "      <td>A brilliantly crafted modern tale from acclaim...</td>\n",
       "      <td>https://www.goodreads.com/book/show/39988.Matilda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>book_2</th>\n",
       "      <td>The Hunger Games</td>\n",
       "      <td>Could you survive on your own in the wild, wit...</td>\n",
       "      <td>https://www.goodreads.com/book/show/2767052-th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 bookTitle                                               Plot  \\\n",
       "book_1  Dreams and Shadows  A brilliantly crafted modern tale from acclaim...   \n",
       "book_2    The Hunger Games  Could you survive on your own in the wild, wit...   \n",
       "\n",
       "                                                      Url  \n",
       "book_1  https://www.goodreads.com/book/show/39988.Matilda  \n",
       "book_2  https://www.goodreads.com/book/show/2767052-th...  "
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Search_Engine(input().split())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
