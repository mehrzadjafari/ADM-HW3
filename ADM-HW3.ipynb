{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 3 - Which book would you recommend?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing related libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import lxml\n",
    "import csv\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "from langdetect import detect\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import PorterStemmer\n",
    "import pandas as pd\n",
    "import glob\n",
    "import pickle\n",
    "import functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Get the list of books"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* From [best books ever list](https://www.goodreads.com/list/show/1.Best_Books_Ever?page=1) we want to collect the url associated to each book in the list and retrieve only the urls of the books listed in the first 300 pages.\n",
    "\n",
    "\n",
    "\n",
    "* The output of this step is a `.txt` file whose single line corresponds to a book's url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inizialize an empty list\n",
    "url = []\n",
    "\n",
    "#for each page, save the corresponding web page, find the anchor elements and save the corresponding tags  \n",
    "for i in range(1, 301):\n",
    "    page = requests.get(\"https://www.goodreads.com/list/show/1.Best_Books_Ever?page=\"+ str(i))\n",
    "    soup = BeautifulSoup(page.content, features='lxml')\n",
    "    tag_a = soup.find_all('a', {\"class\": \"bookTitle\"}, itemprop = \"url\")\n",
    "    \n",
    "#for each book, save the corresponding url into the array\n",
    "    for j in range(0,100):\n",
    "        url.append(\"https://www.goodreads.com\"+ tag_a[j]['href'])\n",
    "        \n",
    "#create a txt file where for each row there is a book's url \n",
    "with open(\"url.txt\", 'w') as f:\n",
    "    f.write(\"\\n\".join(map(str, url)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Crawl books"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Download the html corresponding to each of the collected urls.\n",
    "\n",
    "\n",
    "2. After you collect a single page, immediatly save its `html` in a file. In this way, if your program stops, for any reason, you will not loose the data collected up to the stopping point.\n",
    "\n",
    "\n",
    "3. Organize the entire set of downloaded `html` pages into folders. Each folder will contain the `htmls` of the books in page 1, page 2, ... of the list of books."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warning :\n",
    "\n",
    "### Do not run the below cell because it'll download over 20 GB files !\n",
    "#### Before running modify the `for` loop range to a lower number (e.g. 3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we open the url.txt file, reading the lines and then close the file\n",
    "f = open(\"url.txt\", \"r\")\n",
    "lines = f.readlines()\n",
    "f.close()\n",
    "\n",
    "#Setting our parent directory and the directory associated with each page number from the list\n",
    "## PLEASE CHANGE THE PARENT DIRECTORY ACCORDING TO YOUR SYSTEM\n",
    "parent_dir = \"C:/Users/engme/OneDrive/Desktop/ALL Materials/Data Science - Sapienza/1st Semester/ALGORITHMIC METHODS OF DATA MINING AND LABORATORY/Labs/6/Links\"\n",
    "directory = \"Page_\"\n",
    "page_num = 0\n",
    "\n",
    "\n",
    "#Looping for 300 times that corresponds to the number of pages\n",
    "for i in range(300):\n",
    "    #Incrementing page number according to each page\n",
    "    page_num += 1\n",
    "    #Setting the current working directory\n",
    "    directory = \"Page_\" + str(page_num)\n",
    "    #Setting the main path to create the directory\n",
    "    path = os.path.join(parent_dir, directory)\n",
    "    #Creating new directory\n",
    "    os.makedirs(path)\n",
    "    \n",
    "    #Looping for 100 times, which is the number of articles per page\n",
    "    for i in range(100):\n",
    "        #Selecting the corresponding link\n",
    "        link = lines[i][:-1]\n",
    "        #Dowloading the article\n",
    "        r = requests.get(link, allow_redirects=True)\n",
    "        #Setting the name file to keep the track\n",
    "        file_name = \"article_\" + str(i+1) + \".html\"\n",
    "        #Saving the html file to its corresponding directory\n",
    "        open(parent_dir + \"/\" + directory + \"/\" + file_name, 'wb').write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Parse downloaded pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Extracting the books information for each book as following:\n",
    "\n",
    "1. Title (to save as `bookTitle`)\n",
    "2. Series (to save as `bookSeries`)\n",
    "3. Author(s), the first box in the picture below (to save as `bookAuthors`)\n",
    "4. Ratings, average stars (to save as `ratingValue`)\n",
    "5. Number of givent ratings (to save as `ratingCount`)\n",
    "6. Number of reviews (to save as `reviewCount`)\n",
    "7. The entire plot (to save as `Plot`)\n",
    "8. Number of pages (to save as `NumberofPages`)\n",
    "9. Published (Publishing Date)\n",
    "10. Characters\n",
    "11. Setting\n",
    "12. Url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Custom function designed to \n",
    "parent_dir = \"C:/Users/engme/OneDrive/Desktop/ALL Materials/Data Science - Sapienza/1st Semester/ALGORITHMIC METHODS OF DATA MINING AND LABORATORY/Labs/6\"\n",
    "\n",
    "functions.info_parser(parent_dir, pages=300, tsv_articles= \"tsv_articles\", links= \"Links\", url= 'url')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All tsv files generated sucessfully in tsv_articles directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Search Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to create two different Search Engines that, given as input a query, return the books that match the query.\n",
    "\n",
    "First, you must pre-process all the information collected for each book by :\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Removing stopwords\n",
    "2. Removing punctuation\n",
    "3. Stemming\n",
    "4. Anything else you think it's needed\n",
    "\n",
    "\n",
    "For this purpose, you can use the [nltk](https://www.nltk.org/) library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* At first we want to load all the .tsv files into one .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a .csv files out of all .tsv files\n",
    "'''\n",
    "parent_dir = \"C:/Users/engme/OneDrive/Desktop/ALL Materials/Data Science - Sapienza/1st Semester/ALGORITHMIC METHODS OF DATA MINING AND LABORATORY/Labs/6\"\n",
    "tsv_folder = \"tsv_articles\"\n",
    "\n",
    "'''\n",
    "# The function to create .csv file\n",
    "'''\n",
    "df_tsv = functions.create_csv(parent_dir, tsv_folder, export_csv= False)\n",
    "\n",
    "''';\n",
    "#Reading the created .csv file\n",
    "df_tsv = pd.read_csv(\"files/combined_csv.csv\", usecols=[\"bookTitle\", \"Plot\", \"Url\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Conjunctive query\n",
    "\n",
    "For the first version of the search engine, we narrow our interest on the `Plot` of each document. It means that you will evaluate queries only with respect to the book's plot.\n",
    "\n",
    "**2.1.1) Create your index!**\n",
    "\n",
    "Before building the index,\n",
    "Create a file named `vocabulary`, in the format you prefer, that maps each word to an integer (`term_id`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the inverted index and vocabulary dictionaries\n",
    "'''\n",
    "inv_index1, vocabulary, processed_docs = functions.create_dictionary_plot(df_tsv, export_json = True)\n",
    "\n",
    "''';\n",
    "\n",
    "# Loading the saved dictionaries\n",
    "with open(\"files/dict_file\", \"rb\") as input_file:\n",
    "    inv_index1 = pickle.load(input_file)\n",
    "    input_file.close\n",
    "    \n",
    "with open(\"files/voc_file\", \"rb\") as input_file:\n",
    "    vocabulary = pickle.load(input_file)\n",
    "    input_file.close\n",
    "    \n",
    "with open(\"files/doc_file\", \"rb\") as input_file:\n",
    "    processed_docs = pickle.load(input_file)\n",
    "    input_file.close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1.2) Execute the query**\n",
    "\n",
    "Given a query, that you let the user enter, the Search Engine is supposed to return a list of documents.\n",
    "\n",
    "> What documents do we want?\n",
    "\n",
    "Since we are dealing with conjunctive queries (AND), each of the returned documents should contain all the words in the query. The final output of the query must return, if present, the following information for each of the selected documents:\n",
    "\n",
    "* `bookTitle`\n",
    "* `Plot`\n",
    "* `Url`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please Enter your query: harry potter azkaban\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bookTitle</th>\n",
       "      <th>Plot</th>\n",
       "      <th>Url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23166</th>\n",
       "      <td>The Harry Potter trilogy</td>\n",
       "      <td>This box set collects hard cover editions  Har...</td>\n",
       "      <td>https://www.goodreads.com/book/show/1953.A_Tal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25775</th>\n",
       "      <td>Harry Potter and the Prisoner of Azkaban</td>\n",
       "      <td>For twelve long years, the dread fortress of A...</td>\n",
       "      <td>https://www.goodreads.com/book/show/5.Harry_Po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6198</th>\n",
       "      <td>The Harry Potter Collection 1-4</td>\n",
       "      <td>The exciting tales of Harry Potter, the young ...</td>\n",
       "      <td>https://www.goodreads.com/book/show/968.The_Da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23590</th>\n",
       "      <td>Harry Potter Boxed Set, Books 1-5 (Harry Potte...</td>\n",
       "      <td>Box Set containing Harry Potter and the Sorcer...</td>\n",
       "      <td>https://www.goodreads.com/book/show/3636.The_G...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               bookTitle  \\\n",
       "23166                           The Harry Potter trilogy   \n",
       "25775           Harry Potter and the Prisoner of Azkaban   \n",
       "6198                     The Harry Potter Collection 1-4   \n",
       "23590  Harry Potter Boxed Set, Books 1-5 (Harry Potte...   \n",
       "\n",
       "                                                    Plot  \\\n",
       "23166  This box set collects hard cover editions  Har...   \n",
       "25775  For twelve long years, the dread fortress of A...   \n",
       "6198   The exciting tales of Harry Potter, the young ...   \n",
       "23590  Box Set containing Harry Potter and the Sorcer...   \n",
       "\n",
       "                                                     Url  \n",
       "23166  https://www.goodreads.com/book/show/1953.A_Tal...  \n",
       "25775  https://www.goodreads.com/book/show/5.Harry_Po...  \n",
       "6198   https://www.goodreads.com/book/show/968.The_Da...  \n",
       "23590  https://www.goodreads.com/book/show/3636.The_G...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = input(\"Please Enter your query: \")\n",
    "functions.Search_Engine1(query, df_tsv, vocabulary, inv_index1, results= 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2) Conjunctive query & Ranking score\n",
    "\n",
    "For the second search engine, given a query, we want to get the top-k (the choice of k it's up to you!) documents related to the query. In particular:\n",
    "\n",
    "* Find all the documents that contains all the words in the query.\n",
    "* Sort them by their similarity with the query\n",
    "* Return in output k documents, or all the documents with non-zero similarity with the query when the results are less than k. You must use a heap data structure (you can use Python libraries) for maintaining the top-k documents.\n",
    "\n",
    "To solve this task, you will have to use the tfIdf score, and the Cosine similarity. The fielf to consider it is still the plot. Let's see how.\n",
    "\n",
    "**2.2.1) Inverted index**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "inv_index2 = functions.create_invert_index2(len(df_tsv), inv_index1, processed_docs)\n",
    "dictSimilarity = functions.create_similarity_dictionary(inv_index2)\n",
    "\n",
    "''';\n",
    "\n",
    "\n",
    "with open(\"files/dictsim_file\", \"rb\") as input_file:\n",
    "    dictSimilarity = pickle.load(input_file)\n",
    "    input_file.close\n",
    "\n",
    "with open(\"files/inv2_file\", \"rb\") as input_file:\n",
    "    inv_index2 = pickle.load(input_file)\n",
    "    input_file.close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2.2) Execute the query**\n",
    "\n",
    "In this new setting, given a query you get the right set of documents (i.e., those containing all the words in the query) and sort them according to their similairty to the query. For this purpose, as scoring function we will use the Cosine Similarity with respect to the tfIdf representations of the documents.\n",
    "\n",
    "The search engine is supposed to return a list of documents, ranked by their Cosine Similarity with respect to the query entered in input.\n",
    "\n",
    ">More precisely, the output must contain:\n",
    "\n",
    "* `bookTitle`\n",
    "* `Plot`\n",
    "* `Url`\n",
    "* The similarity score of the documents with respect to the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your query: harry potter azkaban\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bookTitle</th>\n",
       "      <th>Plot</th>\n",
       "      <th>Url</th>\n",
       "      <th>Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23166</th>\n",
       "      <td>The Harry Potter trilogy</td>\n",
       "      <td>This box set collects hard cover editions  Har...</td>\n",
       "      <td>https://www.goodreads.com/book/show/1953.A_Tal...</td>\n",
       "      <td>1.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23590</th>\n",
       "      <td>Harry Potter Boxed Set, Books 1-5 (Harry Potte...</td>\n",
       "      <td>Box Set containing Harry Potter and the Sorcer...</td>\n",
       "      <td>https://www.goodreads.com/book/show/3636.The_G...</td>\n",
       "      <td>1.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6198</th>\n",
       "      <td>The Harry Potter Collection 1-4</td>\n",
       "      <td>The exciting tales of Harry Potter, the young ...</td>\n",
       "      <td>https://www.goodreads.com/book/show/968.The_Da...</td>\n",
       "      <td>1.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25775</th>\n",
       "      <td>Harry Potter and the Prisoner of Azkaban</td>\n",
       "      <td>For twelve long years, the dread fortress of A...</td>\n",
       "      <td>https://www.goodreads.com/book/show/5.Harry_Po...</td>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               bookTitle  \\\n",
       "23166                           The Harry Potter trilogy   \n",
       "23590  Harry Potter Boxed Set, Books 1-5 (Harry Potte...   \n",
       "6198                     The Harry Potter Collection 1-4   \n",
       "25775           Harry Potter and the Prisoner of Azkaban   \n",
       "\n",
       "                                                    Plot  \\\n",
       "23166  This box set collects hard cover editions  Har...   \n",
       "23590  Box Set containing Harry Potter and the Sorcer...   \n",
       "6198   The exciting tales of Harry Potter, the young ...   \n",
       "25775  For twelve long years, the dread fortress of A...   \n",
       "\n",
       "                                                     Url  Similarity  \n",
       "23166  https://www.goodreads.com/book/show/1953.A_Tal...        1.45  \n",
       "23590  https://www.goodreads.com/book/show/3636.The_G...        1.43  \n",
       "6198   https://www.goodreads.com/book/show/968.The_Da...        1.15  \n",
       "25775  https://www.goodreads.com/book/show/5.Harry_Po...        0.93  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = input(\"Enter your query: \")\n",
    "functions.Search_Engine2(query, df_tsv, inv_index1, inv_index2, dictSimilarity, vocabulary, results= 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define a new score!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's your turn. Build a new metric to rank books based on the queries of their users.\n",
    "\n",
    "In this scenario, a single user can give in input more information than the single textual query, so you need to take into account all this information, and think a creative and logical way on how to answer at user's requests.\n",
    "\n",
    "Practically:\n",
    "\n",
    "The user will enter you a text query. As a starting point, get the query-related documents by exploiting the search engine of Step 3.1.\n",
    "\n",
    "Once you have the documents, you need to sort them according to your new score. In this step you won't have anymore to take into account just the `plot` of the documents, you **must** use the remaining variables in your dataset (or new possible variables that you can create from the existing ones...). You **must** use a heap data structure (you can use Python libraries) for maintaining the *top-k* documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Q: How to sort them? A: Allow the user to specify more information, that you find in the documents, and define a new metric that ranks the results based on the new request.\n",
    "\n",
    "**N.B.**: You have to define a **scoring function**, not a filter!\n",
    "\n",
    "\n",
    "The output, must contain:\n",
    "\n",
    "* `bookTitle`\n",
    "* `Plot`\n",
    "* `Url`\n",
    "* The similarity score of the documents with respect to the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
