{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 3 - Which book would you recommend?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing related libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import lxml\n",
    "import csv\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "from langdetect import detect\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import PorterStemmer\n",
    "import pandas as pd\n",
    "import glob\n",
    "import json\n",
    "import functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. Get the list of books"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* From [best books ever list](https://www.goodreads.com/list/show/1.Best_Books_Ever?page=1) we want to collect the url associated to each book in the list and retrieve only the urls of the books listed in the first 300 pages.\n",
    "\n",
    "\n",
    "\n",
    "* The output of this step is a `.txt` file whose single line corresponds to a book's url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inizialize an empty list\n",
    "url = []\n",
    "\n",
    "#for each page, save the corresponding web page, find the anchor elements and save the corresponding tags  \n",
    "for i in range(1, 301):\n",
    "    page = requests.get(\"https://www.goodreads.com/list/show/1.Best_Books_Ever?page=\"+ str(i))\n",
    "    soup = BeautifulSoup(page.content, features='lxml')\n",
    "    tag_a = soup.find_all('a', {\"class\": \"bookTitle\"}, itemprop = \"url\")\n",
    "    \n",
    "#for each book, save the corresponding url into the array\n",
    "    for j in range(0,100):\n",
    "        url.append(\"https://www.goodreads.com\"+ tag_a[j]['href'])\n",
    "        \n",
    "#create a txt file where for each row there is a book's url \n",
    "with open(\"url.txt\", 'w') as f:\n",
    "    f.write(\"\\n\".join(map(str, url)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. Crawl books"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Download the html corresponding to each of the collected urls.\n",
    "\n",
    "\n",
    "2. After you collect a single page, immediatly save its `html` in a file. In this way, if your program stops, for any reason, you will not loose the data collected up to the stopping point.\n",
    "\n",
    "\n",
    "3. Organize the entire set of downloaded `html` pages into folders. Each folder will contain the `htmls` of the books in page 1, page 2, ... of the list of books."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warning :\n",
    "\n",
    "### Do not run the below cell because it'll download over 20 GB files !\n",
    "#### Before running modify the `for` loop range to a lower number (e.g. 3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we open the url.txt file, reading the lines and then close the file\n",
    "f = open(\"url.txt\", \"r\")\n",
    "lines = f.readlines()\n",
    "f.close()\n",
    "\n",
    "#Setting our parent directory and the directory associated with each page number from the list\n",
    "## PLEASE CHANGE THE PARENT DIRECTORY ACCORDING TO YOUR SYSTEM\n",
    "parent_dir = \"C:/Users/engme/OneDrive/Desktop/ALL Materials/Data Science - Sapienza/1st Semester/ALGORITHMIC METHODS OF DATA MINING AND LABORATORY/Labs/6/Links\"\n",
    "directory = \"Page_\"\n",
    "page_num = 0\n",
    "\n",
    "\n",
    "#Looping for 300 times that corresponds to the number of pages\n",
    "for i in range(300):\n",
    "    #Incrementing page number according to each page\n",
    "    page_num += 1\n",
    "    #Setting the current working directory\n",
    "    directory = \"Page_\" + str(page_num)\n",
    "    #Setting the main path to create the directory\n",
    "    path = os.path.join(parent_dir, directory)\n",
    "    #Creating new directory\n",
    "    os.makedirs(path)\n",
    "    \n",
    "    #Looping for 100 times, which is the number of articles per page\n",
    "    for i in range(100):\n",
    "        #Selecting the corresponding link\n",
    "        link = lines[i][:-1]\n",
    "        #Dowloading the article\n",
    "        r = requests.get(link, allow_redirects=True)\n",
    "        #Setting the name file to keep the track\n",
    "        file_name = \"article_\" + str(i+1) + \".html\"\n",
    "        #Saving the html file to its corresponding directory\n",
    "        open(parent_dir + \"/\" + directory + \"/\" + file_name, 'wb').write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Parse downloaded pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Extracting the books information for each book as following:\n",
    "\n",
    "1. Title (to save as `bookTitle`)\n",
    "2. Series (to save as `bookSeries`)\n",
    "3. Author(s), the first box in the picture below (to save as `bookAuthors`)\n",
    "4. Ratings, average stars (to save as `ratingValue`)\n",
    "5. Number of givent ratings (to save as `ratingCount`)\n",
    "6. Number of reviews (to save as `reviewCount`)\n",
    "7. The entire plot (to save as `Plot`)\n",
    "8. Number of pages (to save as `NumberofPages`)\n",
    "9. Published (Publishing Date)\n",
    "10. Characters\n",
    "11. Setting\n",
    "12. Url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Custom function designed to \n",
    "parent_dir = \"C:/Users/engme/OneDrive/Desktop/ALL Materials/Data Science - Sapienza/1st Semester/ALGORITHMIC METHODS OF DATA MINING AND LABORATORY/Labs/6\"\n",
    "\n",
    "functions.info_parser(parent_dir, pages=300, tsv_articles= \"tsv_articles\", links= \"Links\", url= 'url')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All tsv files generated sucessfully in tsv_articles directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Search Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to create two different Search Engines that, given as input a query, return the books that match the query.\n",
    "\n",
    "First, you must pre-process all the information collected for each book by :\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Removing stopwords\n",
    "2. Removing punctuation\n",
    "3. Stemming\n",
    "4. Anything else you think it's needed\n",
    "\n",
    "\n",
    "For this purpose, you can use the [nltk](https://www.nltk.org/) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a .csv files out of all .tsv files\n",
    "\n",
    "#parent_dir = \"C:/Users/engme/OneDrive/Desktop/ALL Materials/Data Science - Sapienza/1st Semester/ALGORITHMIC METHODS OF DATA MINING AND LABORATORY/Labs/6\"\n",
    "#tsv_folder = \"tsv_articles\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# The function to create .csv file\n",
    "\n",
    "####df_tsv = functions.create_csv(parent_dir, tsv_folder, export_csv= False)\n",
    "\n",
    "\n",
    "#Reading the created .csv file\n",
    "df_tsv = pd.read_csv(\"combined_csv.csv\", usecols=[\"bookTitle\", \"Plot\", \"Url\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the inverted index and vocabulary dictionaries\n",
    "\n",
    "inv_index1, vocabulary = functions.create_dictionary_plot(df_tsv, export_json = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please Enter your query: harry potter azkaban\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bookTitle</th>\n",
       "      <th>Plot</th>\n",
       "      <th>Url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6198</th>\n",
       "      <td>The Harry Potter Collection 1-4</td>\n",
       "      <td>The exciting tales of Harry Potter, the young ...</td>\n",
       "      <td>https://www.goodreads.com/book/show/968.The_Da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23166</th>\n",
       "      <td>The Harry Potter trilogy</td>\n",
       "      <td>This box set collects hard cover editions  Har...</td>\n",
       "      <td>https://www.goodreads.com/book/show/1953.A_Tal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25775</th>\n",
       "      <td>Harry Potter and the Prisoner of Azkaban</td>\n",
       "      <td>For twelve long years, the dread fortress of A...</td>\n",
       "      <td>https://www.goodreads.com/book/show/5.Harry_Po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23590</th>\n",
       "      <td>Harry Potter Boxed Set, Books 1-5 (Harry Potte...</td>\n",
       "      <td>Box Set containing Harry Potter and the Sorcer...</td>\n",
       "      <td>https://www.goodreads.com/book/show/3636.The_G...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               bookTitle  \\\n",
       "6198                     The Harry Potter Collection 1-4   \n",
       "23166                           The Harry Potter trilogy   \n",
       "25775           Harry Potter and the Prisoner of Azkaban   \n",
       "23590  Harry Potter Boxed Set, Books 1-5 (Harry Potte...   \n",
       "\n",
       "                                                    Plot  \\\n",
       "6198   The exciting tales of Harry Potter, the young ...   \n",
       "23166  This box set collects hard cover editions  Har...   \n",
       "25775  For twelve long years, the dread fortress of A...   \n",
       "23590  Box Set containing Harry Potter and the Sorcer...   \n",
       "\n",
       "                                                     Url  \n",
       "6198   https://www.goodreads.com/book/show/968.The_Da...  \n",
       "23166  https://www.goodreads.com/book/show/1953.A_Tal...  \n",
       "25775  https://www.goodreads.com/book/show/5.Harry_Po...  \n",
       "23590  https://www.goodreads.com/book/show/3636.The_G...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = input(\"Please Enter your query: \")\n",
    "functions.Search_Engine1(query, df_tsv, vocabulary, inv_index1, results= 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
