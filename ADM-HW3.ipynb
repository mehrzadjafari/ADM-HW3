{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 3 - Which book would you recomend?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing related libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests"
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. Get the list of books"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* From [best books ever list](https://www.goodreads.com/list/show/1.Best_Books_Ever?page=1) we want to collect the url associated to each book in the list and retrieve only the urls of the books listed in the first 300 pages.\n",
    "\n",
    "\n",
    "\n",
    "* The output of this step is a `.txt` file whose single line corresponds to a book's url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inizialize an empty list\n",
    "url = []\n",
    "\n",
    "#for each page, save the corresponding web page, find the anchor elements and save the corresponding tags  \n",
    "for i in range(1, 301):\n",
    "    page = requests.get(\"https://www.goodreads.com/list/show/1.Best_Books_Ever?page=\"+ str(i))\n",
    "    soup = BeautifulSoup(page.content, features='lxml')\n",
    "    tag_a = soup.find_all('a', {\"class\": \"bookTitle\"}, itemprop = \"url\")\n",
    "    \n",
    "#for each book, save the corresponding url into the array\n",
    "    for j in range(0,100):\n",
    "        url.append(\"https://www.goodreads.com\"+ tag_a[j]['href'])\n",
    "        \n",
    "#create a txt file where for each row there is a book's url \n",
    "with open(\"url.txt\", 'w') as f:\n",
    "    f.write(\"\\n\".join(map(str, url)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. Crawl books"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Download the html corresponding to each of the collected urls.\n",
    "\n",
    "\n",
    "2. After you collect a single page, immediatly save its `html` in a file. In this way, if your program stops, for any reason, you will not loose the data collected up to the stopping point.\n",
    "\n",
    "\n",
    "3. Organize the entire set of downloaded `html` pages into folders. Each folder will contain the `htmls` of the books in page 1, page 2, ... of the list of books."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warning :\n",
    "\n",
    "### Do not run the below cell because it'll download over 20 GB files !\n",
    "#### Before running modify the `for` loop range to a lower number (e.g. 3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we open the url.txt file, reading the lines and then close the file\n",
    "f = open(\"url.txt\", \"r\")\n",
    "lines = f.readlines()\n",
    "f.close()\n",
    "\n",
    "#Setting our parent directory and the directory associated with each page number from the list\n",
    "## PLEASE CHANGE THE PARENT DIRECTORY ACCORDING TO YOUR SYSTEM\n",
    "parent_dir = \"C:/Users/engme/OneDrive/Desktop/ALL Materials/Data Science - Sapienza/1st Semester/ALGORITHMIC METHODS OF DATA MINING AND LABORATORY/Labs/6/Links\"\n",
    "directory = \"Page_\"\n",
    "page_num = 0\n",
    "\n",
    "\n",
    "#Looping for 300 times that corresponds to the number of pages\n",
    "for i in range(300):\n",
    "    #Incrementing page number according to each page\n",
    "    page_num += 1\n",
    "    #Setting the current working directory\n",
    "    directory = \"Page_\" + str(page_num)\n",
    "    #Setting the main path to create the directory\n",
    "    path = os.path.join(parent_dir, directory)\n",
    "    #Creating new directory\n",
    "    os.makedirs(path)\n",
    "    \n",
    "    #Looping for 100 times, which is the number of articles per page\n",
    "    for i in range(100):\n",
    "        #Selecting the corresponding link\n",
    "        link = lines[i][:-1]\n",
    "        #Dowloading the article\n",
    "        r = requests.get(link, allow_redirects=True)\n",
    "        #Setting the name file to keep the track\n",
    "        file_name = \"article_\" + str(i+1) + \".html\"\n",
    "        #Saving the html file to its corresponding directory\n",
    "        open(parent_dir + \"/\" + directory + \"/\" + file_name, 'wb').write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3. Parse downloaded pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extracting the books informations each book as the following:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Title (to save as `bookTitle`)\n",
    "2. Series (to save as `bookSeries`)\n",
    "3. Author(s), the first box in the picture below (to save as `bookAuthors`)\n",
    "4. Ratings, average stars (to save as `ratingValue`)\n",
    "5. Number of givent ratings (to save as `ratingCount`)\n",
    "6. Number of reviews (to save as `reviewCount`)\n",
    "7. The entire plot (to save as `Plot`)\n",
    "8. Number of pages (to save as `NumberofPages`)\n",
    "9. Published (Publishing Date)\n",
    "10. Characters\n",
    "11. Setting\n",
    "12. Url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note to colleagues ::\n",
    "\n",
    "\n",
    "**`Plot` is not complete yet, and `Url` will be added after we wanted to initiate the complete loop for data extraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bookTitle = soup.find_all('h1', id = \"bookTitle\")[0].contents[0].replace('\\n', '').strip()\n",
    "bookSeries= soup.find_all('a', {\"class\": \"greyText\"})[0].contents[0].replace('\\n', '').strip()\n",
    "bookAuthors = soup.find_all('a', {\"class\": \"authorName\"})[0].contents[0].contents[0]\n",
    "ratingValue = soup.find_all('span', itemprop = \"ratingValue\")[0].contents[0].replace('\\n', '').strip()\n",
    "ratingCount = soup.find_all('a', href=\"#other_reviews\")[0].contents[2].replace('\\n', '').strip().split()[0]\n",
    "reviewCount = soup.find_all('a', href=\"#other_reviews\")[1].contents[2].replace('\\n', '').strip().split()[0]\n",
    "#Plot = soup.find_all('div', id=\"description\")[0].contents[1].contents[0]\n",
    "NumberofPages = soup.find_all('span', itemprop=\"numberOfPages\")[0].contents[0]\n",
    "Published = soup.find_all('div', {\"class\": \"row\"})[1].contents[0].replace('\\n', '').strip().split(\" \"*8)[1]\n",
    "\n",
    "lst = soup.find_all('div', {\"class\": \"infoBoxRowTitle\"})\n",
    "\n",
    "character_index = False\n",
    "for stri in lst:\n",
    "    if 'Characters' in stri:\n",
    "        character_index = lst.index(stri)\n",
    "\n",
    "characters = []\n",
    "\n",
    "if character_index:\n",
    "\n",
    "    if int(len(soup.find_all('div', {\"class\": \"infoBoxRowItem\"})[character_index].contents)/2) <= 5 :\n",
    "        for i in range(int(len(soup.find_all('div', {\"class\": \"infoBoxRowItem\"})[character_index].contents)/2)):\n",
    "\n",
    "            characters.append(str(soup.find_all('div', {\"class\": \"infoBoxRowItem\"})[character_index].contents[2*i+1]).strip().split(\">\")[1][:-3])\n",
    "    else:\n",
    "        for i in range(5):\n",
    "            characters.append(str(soup.find_all('div', {\"class\": \"infoBoxRowItem\"})[character_index].contents[2*i+1]).strip().split(\">\")[1][:-3])\n",
    "\n",
    "    characters = \", \".join(characters)\n",
    "\n",
    "    \n",
    "setting = []\n",
    "setting_index = False\n",
    "for stril in lst:\n",
    "    if 'Setting' in stril:\n",
    "        setting_index = lst.index(stril)\n",
    "\n",
    "if setting_index:\n",
    "\n",
    "    for i in range(3):\n",
    "\n",
    "        setting.append(str(soup.find_all('div', {\"class\": \"infoBoxRowItem\"})[setting_index].contents[2*i+1]).strip().split(\">\")[1][:-3])\n",
    "\n",
    "    setting = \" \".join(setting)\n",
    "\n",
    "    \n",
    "'''\n",
    "print(bookTitle)\n",
    "print(bookSeries)\n",
    "print(bookAuthors)\n",
    "print(ratingValue)\n",
    "print(ratingCount)\n",
    "print(reviewCount)\n",
    "print(Plot)\n",
    "print(NumberofPages)\n",
    "print(Published)\n",
    "print(characters)\n",
    "print(setting)''';"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
