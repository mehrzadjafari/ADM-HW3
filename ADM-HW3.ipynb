{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 3 - Which book would you recomend?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing related libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests"
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. Get the list of books"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* From [best books ever list](https://www.goodreads.com/list/show/1.Best_Books_Ever?page=1) we want to collect the url associated to each book in the list and retrieve only the urls of the books listed in the first 300 pages.\n",
    "\n",
    "\n",
    "\n",
    "* The output of this step is a `.txt` file whose single line corresponds to a book's url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inizialize an empty list\n",
    "url = []\n",
    "\n",
    "#for each page, save the corresponding web page, find the anchor elements and save the corresponding tags  \n",
    "for i in range(1, 301):\n",
    "    page = requests.get(\"https://www.goodreads.com/list/show/1.Best_Books_Ever?page=\"+ str(i))\n",
    "    soup = BeautifulSoup(page.content, features='lxml')\n",
    "    tag_a = soup.find_all('a', {\"class\": \"bookTitle\"}, itemprop = \"url\")\n",
    "    \n",
    "#for each book, save the corresponding url into the array\n",
    "    for j in range(0,100):\n",
    "        url.append(\"https://www.goodreads.com\"+ tag_a[j]['href'])\n",
    "        \n",
    "#create a txt file where for each row there is a book's url \n",
    "with open(\"url.txt\", 'w') as f:\n",
    "    f.write(\"\\n\".join(map(str, url)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. Crawl books"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Download the html corresponding to each of the collected urls.\n",
    "\n",
    "\n",
    "2. After you collect a single page, immediatly save its `html` in a file. In this way, if your program stops, for any reason, you will not loose the data collected up to the stopping point.\n",
    "\n",
    "\n",
    "3. Organize the entire set of downloaded `html` pages into folders. Each folder will contain the `htmls` of the books in page 1, page 2, ... of the list of books."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warning :\n",
    "\n",
    "### Do not run the below cell because it'll download over 20 GB files !\n",
    "#### Before running modify the `for` loop range to a lower number (e.g. 3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we open the url.txt file, reading the lines and then close the file\n",
    "f = open(\"url.txt\", \"r\")\n",
    "lines = f.readlines()\n",
    "f.close()\n",
    "\n",
    "#Setting our parent directory and the directory associated with each page number from the list\n",
    "## PLEASE CHANGE THE PARENT DIRECTORY ACCORDING TO YOUR SYSTEM\n",
    "parent_dir = \"C:/Users/engme/OneDrive/Desktop/ALL Materials/Data Science - Sapienza/1st Semester/ALGORITHMIC METHODS OF DATA MINING AND LABORATORY/Labs/6/Links\"\n",
    "directory = \"Page_\"\n",
    "page_num = 0\n",
    "\n",
    "\n",
    "#Looping for 300 times that corresponds to the number of pages\n",
    "for i in range(300):\n",
    "    #Incrementing page number according to each page\n",
    "    page_num += 1\n",
    "    #Setting the current working directory\n",
    "    directory = \"Page_\" + str(page_num)\n",
    "    #Setting the main path to create the directory\n",
    "    path = os.path.join(parent_dir, directory)\n",
    "    #Creating new directory\n",
    "    os.makedirs(path)\n",
    "    \n",
    "    #Looping for 100 times, which is the number of articles per page\n",
    "    for i in range(100):\n",
    "        #Selecting the corresponding link\n",
    "        link = lines[i][:-1]\n",
    "        #Dowloading the article\n",
    "        r = requests.get(link, allow_redirects=True)\n",
    "        #Setting the name file to keep the track\n",
    "        file_name = \"article_\" + str(i+1) + \".html\"\n",
    "        #Saving the html file to its corresponding directory\n",
    "        open(parent_dir + \"/\" + directory + \"/\" + file_name, 'wb').write(r.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
