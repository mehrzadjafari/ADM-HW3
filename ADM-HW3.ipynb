{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 3 - Which book would you recomend?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing related libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. Get the list of books"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* From [best books ever list](https://www.goodreads.com/list/show/1.Best_Books_Ever?page=1) we want to collect the url associated to each book in the list and retrieve only the urls of the books listed in the first 300 pages.\n",
    "\n",
    "\n",
    "\n",
    "* The output of this step is a `.txt` file whose single line corresponds to a book's url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inizialize an empty list\n",
    "url = []\n",
    "\n",
    "#for each page, save the corresponding web page, find the anchor elements and save the corresponding tags  \n",
    "for i in range(1, 301):\n",
    "    page = requests.get(\"https://www.goodreads.com/list/show/1.Best_Books_Ever?page=\"+ str(i))\n",
    "    soup = BeautifulSoup(page.content, features='lxml')\n",
    "    tag_a = soup.find_all('a', {\"class\": \"bookTitle\"}, itemprop = \"url\")\n",
    "    \n",
    "#for each book, save the corresponding url into the array\n",
    "    for j in range(0,100):\n",
    "        url.append(\"https://www.goodreads.com\"+ tag_a[j]['href'])\n",
    "        \n",
    "#create a txt file where for each row there is a book's url \n",
    "with open(\"url.txt\", 'w') as f:\n",
    "    f.write(\"\\n\".join(map(str, url)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. Crawl books"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Download the html corresponding to each of the collected urls.\n",
    "\n",
    "\n",
    "2. After you collect a single page, immediatly save its `html` in a file. In this way, if your program stops, for any reason, you will not loose the data collected up to the stopping point.\n",
    "\n",
    "\n",
    "3. Organize the entire set of downloaded `html` pages into folders. Each folder will contain the `htmls` of the books in page 1, page 2, ... of the list of books."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warning :\n",
    "\n",
    "### Do not run the below cell because it'll download over 20 GB files !\n",
    "#### Before running modify the `for` loop range to a lower number (e.g. 3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we open the url.txt file, reading the lines and then close the file\n",
    "f = open(\"url.txt\", \"r\")\n",
    "lines = f.readlines()\n",
    "f.close()\n",
    "\n",
    "#Setting our parent directory and the directory associated with each page number from the list\n",
    "## PLEASE CHANGE THE PARENT DIRECTORY ACCORDING TO YOUR SYSTEM\n",
    "parent_dir = \"C:/Users/engme/OneDrive/Desktop/ALL Materials/Data Science - Sapienza/1st Semester/ALGORITHMIC METHODS OF DATA MINING AND LABORATORY/Labs/6/Links\"\n",
    "directory = \"Page_\"\n",
    "page_num = 0\n",
    "\n",
    "\n",
    "#Looping for 300 times that corresponds to the number of pages\n",
    "for i in range(300):\n",
    "    #Incrementing page number according to each page\n",
    "    page_num += 1\n",
    "    #Setting the current working directory\n",
    "    directory = \"Page_\" + str(page_num)\n",
    "    #Setting the main path to create the directory\n",
    "    path = os.path.join(parent_dir, directory)\n",
    "    #Creating new directory\n",
    "    os.makedirs(path)\n",
    "    \n",
    "    #Looping for 100 times, which is the number of articles per page\n",
    "    for i in range(100):\n",
    "        #Selecting the corresponding link\n",
    "        link = lines[i][:-1]\n",
    "        #Dowloading the article\n",
    "        r = requests.get(link, allow_redirects=True)\n",
    "        #Setting the name file to keep the track\n",
    "        file_name = \"article_\" + str(i+1) + \".html\"\n",
    "        #Saving the html file to its corresponding directory\n",
    "        open(parent_dir + \"/\" + directory + \"/\" + file_name, 'wb').write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Parse downloaded pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Extracting the books information for each book as following:\n",
    "\n",
    "1. Title (to save as `bookTitle`)\n",
    "2. Series (to save as `bookSeries`)\n",
    "3. Author(s), the first box in the picture below (to save as `bookAuthors`)\n",
    "4. Ratings, average stars (to save as `ratingValue`)\n",
    "5. Number of givent ratings (to save as `ratingCount`)\n",
    "6. Number of reviews (to save as `reviewCount`)\n",
    "7. The entire plot (to save as `Plot`)\n",
    "8. Number of pages (to save as `NumberofPages`)\n",
    "9. Published (Publishing Date)\n",
    "10. Characters\n",
    "11. Setting\n",
    "12. Url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note to colleages: (Temporary)\n",
    "\n",
    "* The `Url` will be added after\n",
    "* The `article_i.tsv` file will be created after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Book Title\n",
    "bookTitle = soup.find_all('h1', id = \"bookTitle\")[0].contents[0].replace('\\n', '').strip()\n",
    "\n",
    "#Book Series\n",
    "series_index = False\n",
    "for stri in lst:\n",
    "    if 'Series' in stri:\n",
    "        series_index = lst.index(stri)\n",
    "\n",
    "bookSeries = \"\"\n",
    "\n",
    "if character_index:\n",
    "\n",
    "            bookSeries += str(soup.find_all('div', {\"class\": \"infoBoxRowItem\"})[series_index].contents[1]).strip().split(\">\")[1][:-3]\n",
    "\n",
    "        \n",
    "\n",
    "#Book Authors\n",
    "bookAuthors = soup.find_all('a', {\"class\": \"authorName\"})[0].contents[0].contents[0]\n",
    "\n",
    "#Rating Value\n",
    "ratingValue = soup.find_all('span', itemprop = \"ratingValue\")[0].contents[0].replace('\\n', '').strip()\n",
    "\n",
    "#Rating Counts\n",
    "ratingCount = soup.find_all('a', href=\"#other_reviews\")[0].contents[2].replace('\\n', '').strip().split()[0]\n",
    "\n",
    "#Review Counts\n",
    "reviewCount = soup.find_all('a', href=\"#other_reviews\")[1].contents[2].replace('\\n', '').strip().split()[0]\n",
    "\n",
    "\n",
    "\n",
    "#Plot\n",
    "\n",
    "#first of all, define the full description \n",
    "#(which may contain italics, bolds and preliminar descriptions not related to the plot)\n",
    "full_descr = soup.find_all('div', id=\"description\")[0].contents[3].contents\n",
    "\n",
    "#if the description begins with this kind of description, remove it\n",
    "if \"<i>\" in str(full_descr[0]):\n",
    "    \n",
    "    full_descr = full_descr[3:]  \n",
    "\n",
    "#for every line of the description, add to the list Plot all of the lines except the ones corresponding to the <br/> tags \n",
    "Plot = []\n",
    "\n",
    "for i in range(len(full_descr)):\n",
    "    if str(full_descr[i]) != '<br/>':\n",
    "        Plot.append(full_descr[i])\n",
    "\n",
    "\n",
    "#for every line of the plot, remove the <i> and </i> tags\n",
    "for i in range(len(Plot)):\n",
    "    if '<i>' in str(Plot[i]):\n",
    "        Plot[i] = re.sub(r'<i>', '', str(Plot[i]))\n",
    "        Plot[i] = re.sub(r'</i>', '', str(Plot[i]))\n",
    "\n",
    "#if the plot begins in bold, remove the <b> tags\n",
    "if \"<b>\" in str(Plot[0]):\n",
    "    Plot[0] = Plot[0].contents[0]\n",
    "    \n",
    "Plot = \" \".join(Plot)\n",
    "\n",
    "\n",
    "\n",
    "#Number of Pages\n",
    "NumberofPages = soup.find_all('span', itemprop=\"numberOfPages\")[0].contents[0]\n",
    "\n",
    "#Published\n",
    "Published = soup.find_all('div', {\"class\": \"row\"})[1].contents[0].replace('\\n', '').strip().split(\" \"*8)[1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Characters\n",
    "lst = soup.find_all('div', {\"class\": \"infoBoxRowTitle\"})\n",
    "\n",
    "#Finding the position of Characters in \"InfoBox\" whether it exists or not\n",
    "character_index = False\n",
    "for stri in lst:\n",
    "    if 'Characters' in stri:\n",
    "        character_index = lst.index(stri)\n",
    "\n",
    "#Initiating the characters list\n",
    "characters = []\n",
    "\n",
    "#If character exists, we will find them up to 5 characters.\n",
    "if character_index:\n",
    "\n",
    "    if int(len(soup.find_all('div', {\"class\": \"infoBoxRowItem\"})[character_index].contents)/2) <= 5 :\n",
    "        for i in range(int(len(soup.find_all('div', {\"class\": \"infoBoxRowItem\"})[character_index].contents)/2)):\n",
    "            \n",
    "            #Using 2i+1 as each character is on odd index contents\n",
    "            characters.append(str(soup.find_all('div', {\"class\": \"infoBoxRowItem\"})[character_index].contents[2*i+1]).strip().split(\">\")[1][:-3])\n",
    "    else:\n",
    "        for i in range(5):\n",
    "            characters.append(str(soup.find_all('div', {\"class\": \"infoBoxRowItem\"})[character_index].contents[2*i+1]).strip().split(\">\")[1][:-3])\n",
    "\n",
    "    characters = \", \".join(characters)\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "#Setting\n",
    "setting = []\n",
    "\n",
    "#Finding the index of Setting whether it exists or not !\n",
    "setting_index = False\n",
    "for stril in lst:\n",
    "    if 'Setting' in stril:\n",
    "        setting_index = lst.index(stril)\n",
    "\n",
    "#Same approach as for characters\n",
    "if setting_index:\n",
    "\n",
    "    for i in range(3):\n",
    "\n",
    "        setting.append(str(soup.find_all('div', {\"class\": \"infoBoxRowItem\"})[setting_index].contents[2*i+1]).strip().split(\">\")[1][:-3])\n",
    "\n",
    "    setting = \" \".join(setting)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
